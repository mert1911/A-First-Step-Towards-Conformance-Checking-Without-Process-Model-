{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7468e092",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099660fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "\n",
    "# install once necessary libraries\n",
    "\n",
    "!pip install pandas\n",
    "!pip install -U scikit-learn\n",
    "!pip install pm4py\n",
    "!pip install kneed\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e7954",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5ae4a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>travel_start</th>\n",
       "      <th>travel_end</th>\n",
       "      <th>case</th>\n",
       "      <th>cost</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>@@index</th>\n",
       "      <th>@@case_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2017-01-17 11:40:00+00:00</td>\n",
       "      <td>Travel Department</td>\n",
       "      <td>KS9688</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>2017-01-18 06:31:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55804</th>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>2017-11-22 06:50:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>55804</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55805</th>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>2017-11-22 13:06:00+00:00</td>\n",
       "      <td>Manager</td>\n",
       "      <td>AK7488</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>55805</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55806</th>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>2017-11-29 20:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>55806</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55807</th>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>2017-12-08 09:55:00+00:00</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>55807</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55808</th>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>2017-12-18 08:52:00+00:00</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>55808</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55809 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                activity  \\\n",
       "0                                    file travel request   \n",
       "1      check if travel request needs preliminary pric...   \n",
       "2                        decide on approval requirements   \n",
       "3                          check if booking is necessary   \n",
       "4                       check if expense documents exist   \n",
       "...                                                  ...   \n",
       "55804                      confirm travel expense report   \n",
       "55805                  decide on travel expense approval   \n",
       "55806                 send original documents to archive   \n",
       "55807                                 calculate payments   \n",
       "55808                                       pay expenses   \n",
       "\n",
       "                          start                       end               type  \\\n",
       "0     2017-01-17 11:17:00+00:00 2017-01-17 11:23:00+00:00           Employee   \n",
       "1     2017-01-17 11:23:00+00:00 2017-01-17 11:24:00+00:00           Employee   \n",
       "2     2017-01-17 11:24:00+00:00 2017-01-17 11:24:00+00:00           Employee   \n",
       "3     2017-01-17 11:24:00+00:00 2017-01-17 11:40:00+00:00  Travel Department   \n",
       "4     2017-01-18 05:59:00+00:00 2017-01-18 06:31:00+00:00           Employee   \n",
       "...                         ...                       ...                ...   \n",
       "55804 2017-11-22 06:48:00+00:00 2017-11-22 06:50:00+00:00           Employee   \n",
       "55805 2017-11-22 12:59:00+00:00 2017-11-22 13:06:00+00:00            Manager   \n",
       "55806 2017-11-29 20:12:00+00:00 2017-11-29 20:24:00+00:00           Employee   \n",
       "55807 2017-12-08 09:32:00+00:00 2017-12-08 09:55:00+00:00         Accounting   \n",
       "55808 2017-12-18 08:39:00+00:00 2017-12-18 08:52:00+00:00         Accounting   \n",
       "\n",
       "         user              travel_start                travel_end  case  cost  \\\n",
       "0      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "1      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "2      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "3      KS9688 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "4      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "...       ...                       ...                       ...   ...   ...   \n",
       "55804  KI9211 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55805  AK7488 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55806  KI9211 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55807  FQ3758 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55808  FQ3758 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "\n",
       "      case:concept:name                                       concept:name  \\\n",
       "0                   105                                file travel request   \n",
       "1                   105  check if travel request needs preliminary pric...   \n",
       "2                   105                    decide on approval requirements   \n",
       "3                   105                      check if booking is necessary   \n",
       "4                   105                   check if expense documents exist   \n",
       "...                 ...                                                ...   \n",
       "55804              6348                      confirm travel expense report   \n",
       "55805              6348                  decide on travel expense approval   \n",
       "55806              6348                 send original documents to archive   \n",
       "55807              6348                                 calculate payments   \n",
       "55808              6348                                       pay expenses   \n",
       "\n",
       "                 time:timestamp  @@index  @@case_index  \n",
       "0     2017-01-17 11:17:00+00:00        0             0  \n",
       "1     2017-01-17 11:23:00+00:00        1             0  \n",
       "2     2017-01-17 11:24:00+00:00        2             0  \n",
       "3     2017-01-17 11:24:00+00:00        3             0  \n",
       "4     2017-01-18 05:59:00+00:00        4             0  \n",
       "...                         ...      ...           ...  \n",
       "55804 2017-11-22 06:48:00+00:00    55804          3353  \n",
       "55805 2017-11-22 12:59:00+00:00    55805          3353  \n",
       "55806 2017-11-29 20:12:00+00:00    55806          3353  \n",
       "55807 2017-12-08 09:32:00+00:00    55807          3353  \n",
       "55808 2017-12-18 08:39:00+00:00    55808          3353  \n",
       "\n",
       "[55809 rows x 14 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe = pd.read_csv('mobis.csv', sep=',')  \n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe = dataframe.drop(dataframe.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe = pm4py.format_dataframe(\n",
    "        dataframe, \n",
    "        case_id='case', \n",
    "        activity_key='activity', \n",
    "        timestamp_key='start'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe)\n",
    "    \n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198eb53",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b64fa",
   "metadata": {},
   "source": [
    "## Integer Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81331e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>travel_start</th>\n",
       "      <th>travel_end</th>\n",
       "      <th>case</th>\n",
       "      <th>cost</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>@@index</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>activity_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2017-01-17 11:40:00+00:00</td>\n",
       "      <td>Travel Department</td>\n",
       "      <td>KS9688</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>2017-01-18 06:31:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55804</th>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>2017-11-22 06:50:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>55804</td>\n",
       "      <td>3353</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55805</th>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>2017-11-22 13:06:00+00:00</td>\n",
       "      <td>Manager</td>\n",
       "      <td>AK7488</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>55805</td>\n",
       "      <td>3353</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55806</th>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>2017-11-29 20:24:00+00:00</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>55806</td>\n",
       "      <td>3353</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55807</th>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>2017-12-08 09:55:00+00:00</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>55807</td>\n",
       "      <td>3353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55808</th>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>2017-12-18 08:52:00+00:00</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>55808</td>\n",
       "      <td>3353</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55809 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                activity  \\\n",
       "0                                    file travel request   \n",
       "1      check if travel request needs preliminary pric...   \n",
       "2                        decide on approval requirements   \n",
       "3                          check if booking is necessary   \n",
       "4                       check if expense documents exist   \n",
       "...                                                  ...   \n",
       "55804                      confirm travel expense report   \n",
       "55805                  decide on travel expense approval   \n",
       "55806                 send original documents to archive   \n",
       "55807                                 calculate payments   \n",
       "55808                                       pay expenses   \n",
       "\n",
       "                          start                       end               type  \\\n",
       "0     2017-01-17 11:17:00+00:00 2017-01-17 11:23:00+00:00           Employee   \n",
       "1     2017-01-17 11:23:00+00:00 2017-01-17 11:24:00+00:00           Employee   \n",
       "2     2017-01-17 11:24:00+00:00 2017-01-17 11:24:00+00:00           Employee   \n",
       "3     2017-01-17 11:24:00+00:00 2017-01-17 11:40:00+00:00  Travel Department   \n",
       "4     2017-01-18 05:59:00+00:00 2017-01-18 06:31:00+00:00           Employee   \n",
       "...                         ...                       ...                ...   \n",
       "55804 2017-11-22 06:48:00+00:00 2017-11-22 06:50:00+00:00           Employee   \n",
       "55805 2017-11-22 12:59:00+00:00 2017-11-22 13:06:00+00:00            Manager   \n",
       "55806 2017-11-29 20:12:00+00:00 2017-11-29 20:24:00+00:00           Employee   \n",
       "55807 2017-12-08 09:32:00+00:00 2017-12-08 09:55:00+00:00         Accounting   \n",
       "55808 2017-12-18 08:39:00+00:00 2017-12-18 08:52:00+00:00         Accounting   \n",
       "\n",
       "         user              travel_start                travel_end  case  cost  \\\n",
       "0      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "1      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "2      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "3      KS9688 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "4      JB8510 2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "...       ...                       ...                       ...   ...   ...   \n",
       "55804  KI9211 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55805  AK7488 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55806  KI9211 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55807  FQ3758 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55808  FQ3758 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "\n",
       "      case:concept:name                                       concept:name  \\\n",
       "0                   105                                file travel request   \n",
       "1                   105  check if travel request needs preliminary pric...   \n",
       "2                   105                    decide on approval requirements   \n",
       "3                   105                      check if booking is necessary   \n",
       "4                   105                   check if expense documents exist   \n",
       "...                 ...                                                ...   \n",
       "55804              6348                      confirm travel expense report   \n",
       "55805              6348                  decide on travel expense approval   \n",
       "55806              6348                 send original documents to archive   \n",
       "55807              6348                                 calculate payments   \n",
       "55808              6348                                       pay expenses   \n",
       "\n",
       "                 time:timestamp  @@index  @@case_index  activity_encoded  \n",
       "0     2017-01-17 11:17:00+00:00        0             0                15  \n",
       "1     2017-01-17 11:23:00+00:00        1             0                 7  \n",
       "2     2017-01-17 11:24:00+00:00        2             0                11  \n",
       "3     2017-01-17 11:24:00+00:00        3             0                 3  \n",
       "4     2017-01-18 05:59:00+00:00        4             0                 4  \n",
       "...                         ...      ...           ...               ...  \n",
       "55804 2017-11-22 06:48:00+00:00    55804          3353                 8  \n",
       "55805 2017-11-22 12:59:00+00:00    55805          3353                13  \n",
       "55806 2017-11-29 20:12:00+00:00    55806          3353                21  \n",
       "55807 2017-12-08 09:32:00+00:00    55807          3353                 1  \n",
       "55808 2017-12-18 08:39:00+00:00    55808          3353                17  \n",
       "\n",
       "[55809 rows x 15 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "dataframe['activity_encoded'] = label_encoder.fit_transform(dataframe['activity'])\n",
    "\n",
    "# Now the 'dataframe' has a new column 'activity_encoded' with integer encoded values of the 'activity' column\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2687d00",
   "metadata": {},
   "source": [
    "## Extract Traces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bb77562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@@case_index\n",
       "0             [15, 7, 11, 3, 4, 25, 14, 8, 13, 21, 1, 17]\n",
       "1       [15, 7, 18, 20, 2, 5, 19, 18, 20, 2, 24, 11, 3...\n",
       "2       [15, 7, 11, 3, 18, 20, 2, 0, 4, 25, 14, 8, 13,...\n",
       "3       [15, 7, 11, 3, 18, 20, 2, 6, 19, 18, 20, 2, 0,...\n",
       "4       [15, 7, 18, 20, 2, 24, 11, 3, 4, 14, 8, 13, 23...\n",
       "                              ...                        \n",
       "3349              [15, 7, 11, 3, 4, 14, 8, 13, 21, 1, 17]\n",
       "3350    [15, 7, 11, 3, 4, 25, 14, 8, 13, 23, 10, 13, 2...\n",
       "3351    [15, 7, 11, 16, 12, 3, 4, 25, 14, 8, 13, 21, 1...\n",
       "3352    [15, 7, 11, 16, 12, 3, 4, 25, 14, 8, 13, 21, 1...\n",
       "3353              [15, 7, 11, 3, 4, 14, 8, 13, 21, 1, 17]\n",
       "Name: activity_encoded, Length: 3354, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by case index to get individual traces\n",
    "traces = dataframe.groupby('@@case_index')['activity_encoded'].apply(list)\n",
    "traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a56959",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8246aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15,  7, 11,  ...,  0,  0,  0],\n",
       "        [15,  7, 18,  ...,  0,  0,  0],\n",
       "        [15,  7, 11,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [15,  7, 11,  ...,  0,  0,  0],\n",
       "        [15,  7, 11,  ...,  0,  0,  0],\n",
       "        [15,  7, 11,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post-Padding\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# First, we convert the pandas Series 'traces' to a list of PyTorch tensors\n",
    "traces_tensors = [torch.tensor(trace) for trace in traces]\n",
    "\n",
    "# Then, we apply padding to this list of tensors\n",
    "# We are assuming here that your traces are sequences of integers that represent encoded activities\n",
    "# pad_sequence requires the input to be a list (or other iterable) of tensors\n",
    "padded_traces = pad_sequence(traces_tensors, batch_first=True)\n",
    "\n",
    "# Now 'padded_traces' is a tensor with each row representing a trace and zeros padding the sequences to equal length\n",
    "padded_traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0456bb2",
   "metadata": {},
   "source": [
    "# Build Input X and output Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f37463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'padded_traces' is already created as per the previous discussion.\n",
    "\n",
    "# Determine the integer for 'END' token, assuming it's the max(encoded_activities) + 1\n",
    "end_token = padded_traces.max() + 1\n",
    "\n",
    "# Initialize lists to hold the input sequences X and the targets Y\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Iterate through each trace\n",
    "for trace in padded_traces:\n",
    "    # Initialize the sequence with the 'null' token\n",
    "    seq = [0]  \n",
    "    for activity in trace:\n",
    "        # Exclude padding (0) and end token (if already included)\n",
    "        if activity.item() not in (0, end_token):\n",
    "            # Append the current sequence to X\n",
    "            X.append(seq.copy())\n",
    "            # Append the next activity as the target to Y\n",
    "            Y.append(activity.item())\n",
    "            # Add the current activity to the sequence\n",
    "            seq.append(activity.item())\n",
    "    # After the last activity in the trace, append the 'END' token to the sequence and to Y\n",
    "    X.append(seq.copy())\n",
    "    Y.append(end_token)\n",
    "\n",
    "# Convert X and Y to PyTorch tensors\n",
    "X_tensor = pad_sequence([torch.tensor(x) for x in X], batch_first=True, padding_value=0)\n",
    "Y_tensor = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19fa48f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([57673, 49])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ed440",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc99776",
   "metadata": {},
   "source": [
    "# Optional: Prepare Test and Train Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60dc4dc7",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert tensors to numpy arrays for train_test_split function\n",
    "X_numpy = X_tensor.numpy()\n",
    "Y_numpy = Y_tensor.numpy()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_numpy, Y_numpy, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert them back to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long).to(device)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca1ae2da",
   "metadata": {},
   "source": [
    "X_tensor = X_train_tensor\n",
    "Y_tensor = Y_train_tensor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0db3465",
   "metadata": {},
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dfb0958a",
   "metadata": {},
   "source": [
    "Y_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264de28",
   "metadata": {},
   "source": [
    "# LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ff032e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_activities, embedding_dim, hidden_dim, output_dim, dropout_prob=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Output of the last LSTM cell\n",
    "        return out, h_n\n",
    "\n",
    "# Model parameters\n",
    "num_activities = end_token + 1 # define this based on your data\n",
    "embedding_dim = len(dataframe['activity'].unique())\n",
    "hidden_dim = 100\n",
    "output_dim = num_activities\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lstm_model = LSTMModel(num_activities, embedding_dim, hidden_dim, output_dim, dropout_prob=0.2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c97a8f",
   "metadata": {},
   "source": [
    "# LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e3f3830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 3.1406667232513428\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters())\n",
    "\n",
    "# Data preparation\n",
    "train_data = TensorDataset(X_tensor, Y_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = lstm_model(inputs)  # Ignore hidden states during training\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c448e",
   "metadata": {},
   "source": [
    "# Optional: Evaluation of LSTM Predictive Performance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d9543ee",
   "metadata": {},
   "source": [
    "# Test data loader\n",
    "test_data = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=32)  # Adjust the batch size if necessary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce095053",
   "metadata": {},
   "source": [
    "# Put the model in evaluation mode\n",
    "lstm_model.eval()\n",
    "\n",
    "# Initialize variables to track predictions and actuals\n",
    "total_predictions = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "# Disable gradient computation during evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs, _ = lstm_model(inputs)\n",
    "\n",
    "        # Convert outputs to predicted class labels\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update total and correct predictions\n",
    "        total_predictions += targets.size(0)\n",
    "        correct_predictions += (predicted_labels == targets).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Accuracy for predicting next activity: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bba543",
   "metadata": {},
   "source": [
    "# Extract Trace Representations Using LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d416e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To collect hidden states for the entire dataset\n",
    "lstm_model.eval()  # Set the model to evaluation mode\n",
    "hidden_states = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs in DataLoader(X_tensor, batch_size=1):  # Process one sequence at a time\n",
    "        inputs = inputs.to(device)\n",
    "        _, h_n = lstm_model(inputs)  # Get the hidden state\n",
    "        hidden_states.append(h_n.squeeze(0).cpu().numpy())  # Remove batch dimension and convert to numpy\n",
    "\n",
    "hidden_states = np.array(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de70f2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57673, 1, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87771f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57673, 100)\n"
     ]
    }
   ],
   "source": [
    "hidden_states = hidden_states.squeeze(1)  # Removes the singleton dimension\n",
    "\n",
    "# Check the new shape\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d476f4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@@case_index\n",
       "0       [file travel request, check if travel request ...\n",
       "1       [file travel request, check if travel request ...\n",
       "2       [file travel request, check if travel request ...\n",
       "3       [file travel request, check if travel request ...\n",
       "4       [file travel request, check if travel request ...\n",
       "                              ...                        \n",
       "3349    [file travel request, check if travel request ...\n",
       "3350    [file travel request, check if travel request ...\n",
       "3351    [file travel request, check if travel request ...\n",
       "3352    [file travel request, check if travel request ...\n",
       "3353    [file travel request, check if travel request ...\n",
       "Name: activity, Length: 3354, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by case index to get individual traces\n",
    "traces = dataframe.groupby('@@case_index')['activity'].apply(list)\n",
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4aea6611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 23, 15, ..., 13, 13, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state_pos = traces.apply(len).values\n",
    "hidden_state_pos = hidden_state_pos - 1\n",
    "hidden_state_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b8f00bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3354"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_representations = [hidden_states[i-1] for i in hidden_state_pos]\n",
    "trace_representations = np.array(trace_representations)\n",
    "len(trace_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa3a50cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3354, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4245f4",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33175a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Elbow method to determine the optimal number of clusters\n",
    "distortions = []\n",
    "K = range(1, 10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(trace_representations)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Find the elbow point\n",
    "kl = KneeLocator(range(1, 10), distortions, curve=\"convex\", direction=\"decreasing\")\n",
    "print(\"Optimal number of clusters:\", kl.elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737848c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "k = kl.elbow\n",
    "\n",
    "# Create a KMeans instance with k clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(trace_representations)\n",
    "\n",
    "# Predict the clusters for each trace\n",
    "clusters = kmeans.predict(trace_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f4834",
   "metadata": {},
   "source": [
    "# Cluster Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea21f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value of the silhouette coefficient ranges between -1 and 1\n",
    "# value close to 1 is considered as good\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(trace_representations, clusters)\n",
    "\n",
    "print(f\"Silhouette Coefficient: {silhouette_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0556d66",
   "metadata": {},
   "source": [
    "# Token-Based Replay for Conformance Checking"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb8bc84e",
   "metadata": {},
   "source": [
    "#import pm4py\n",
    "#from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "#from pm4py.objects.conversion.bpmn import converter as bpmn_converter\n",
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_replay\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"MobisToBe.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, im, fm = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "# 4. Perform conformance checking\n",
    "replayed_traces = token_replay.apply(log, net, im, fm)\n",
    "\n",
    "# Calculate and print diagnostics\n",
    "fit_traces = sum(1 for trace in replayed_traces if trace['trace_is_fit'])\n",
    "\n",
    "print(f\"Total traces: {len(log)}\")\n",
    "print(f\"Conform traces: {fit_traces}\")\n",
    "print(f\"Non-Conform traces: {len(log) - fit_traces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "056854ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a038318efebb49d195e1d6c10991914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total traces: 3354\n",
      "Conform traces: 1690\n",
      "Non-Conform traces: 1664\n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"MobisToBe.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, im, fm = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "# 4. Perform alignment-based conformance checking\n",
    "alignments = alignments_petri.apply(log, net, im, fm)\n",
    "\n",
    "# Calculate and print diagnostics\n",
    "fit_traces = sum(1 for trace in alignments if trace['fitness'] == 1.0)\n",
    "\n",
    "print(f\"Total traces: {len(log)}\")\n",
    "print(f\"Conform traces: {fit_traces}\")\n",
    "print(f\"Non-Conform traces: {len(log) - fit_traces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255155be",
   "metadata": {},
   "source": [
    "# Happy Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "\n",
    "# Identify the dominant cluster\n",
    "from collections import Counter\n",
    "counter = Counter(labels)\n",
    "happy_cluster = counter.most_common(1)[0][0]\n",
    "print(f\"Happy Cluster: {happy_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the data points belonging to the happy cluster\n",
    "happy_cluster_indices = np.where(clusters == happy_cluster)[0]\n",
    "\n",
    "# Use these indices to fetch the corresponding data points from the original data X\n",
    "happy_cluster_data_points = trace_representations[happy_cluster_indices, :]\n",
    "\n",
    "# Calculate the centroid as the mean of these data points\n",
    "happy_cluster_centroid = np.mean(happy_cluster_data_points, axis=0)\n",
    "\n",
    "print(\"Centroid of happy cluster:\", happy_cluster_centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108b9a5",
   "metadata": {},
   "source": [
    "# Distance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1997c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Calculate the Euclidean distance from each data point in X to the happy_cluster_centroid\n",
    "distances_to_centroid = [euclidean(trace_representations[i], happy_cluster_centroid) for i in range(len(trace_representations))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac090cc2",
   "metadata": {},
   "source": [
    "# Results overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f020556",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create dataframe with necessary information for distance measurement\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#grouped = dataframe.groupby('@@case_index')['concept:name'].apply(list).reset_index(name='trace')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(grouped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clusters\n\u001b[1;32m      9\u001b[0m conformity_array \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(trace[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrace_is_fit\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m replayed_traces]\n\u001b[1;32m     10\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconform\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m conformity_array\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "# create dataframe with necessary information for distance measurement\n",
    "\n",
    "grouped = dataframe.groupby('@@case_index')['concept:name'].apply(list).reset_index(name='trace')\n",
    "\n",
    "results = pd.DataFrame(grouped['trace'])\n",
    "\n",
    "results['cluster'] = clusters\n",
    "\n",
    "conformity_array = [int(trace['fitness']) for trace in alignments]\n",
    "results['conform'] = conformity_array\n",
    "\n",
    "results['distance'] = distances_to_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bad1703",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m      2\u001b[0m     count\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mNamedAgg(column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m'\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      3\u001b[0m     conform_count\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mNamedAgg(column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconform\u001b[39m\u001b[38;5;124m'\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mconform_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m summary\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/pandas/core/frame.py:8872\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8872\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   8873\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8874\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[1;32m   8875\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   8876\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   8877\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[1;32m   8878\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m   8879\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m   8880\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[1;32m   8881\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[1;32m   8882\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1274\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1274\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[1;32m   1275\u001b[0m         obj,\n\u001b[1;32m   1276\u001b[0m         keys,\n\u001b[1;32m   1277\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   1278\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   1279\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m   1280\u001b[0m         observed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[1;32m   1281\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cluster'"
     ]
    }
   ],
   "source": [
    "summary = results.groupby('cluster').agg(\n",
    "    count=pd.NamedAgg(column='trace', aggfunc='size'),\n",
    "    conform_count=pd.NamedAgg(column='conform', aggfunc='sum')\n",
    ").reset_index()\n",
    "\n",
    "print(\"cluster\\tcount\\tconform_count\")\n",
    "for _, row in summary.iterrows():\n",
    "    print(f\"{row['cluster']}\\t{row['count']}\\t{row['conform_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1ab2e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Filter the DataFrame into conforming and non-conforming subsets\n",
    "conforming_distances = results[results['conform'] == 1]['distance']\n",
    "non_conforming_distances = results[results['conform'] == 0]['distance']\n",
    "\n",
    "# Determine common bin edges\n",
    "min_distance = min(results['distance'])\n",
    "max_distance = max(results['distance'])\n",
    "bin_edges = np.linspace(min_distance, max_distance, num=30)\n",
    "\n",
    "# Combine the data and reshape for k-means\n",
    "all_distances = results['distance']\n",
    "all_distances = np.array(all_distances)\n",
    "all_distances_reshaped = all_distances.reshape(-1, 1)\n",
    "\n",
    "# Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(all_distances_reshaped)\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Find the threshold as the average of the two cluster centers\n",
    "threshold_value = np.mean(kmeans.cluster_centers_)\n",
    "\n",
    "# Plot histograms and the threshold\n",
    "plt.hist(conforming_distances, bins=bin_edges, alpha=0.5, label='Conforming', color='green')\n",
    "plt.hist(non_conforming_distances, bins=bin_edges, alpha=0.5, label='Non-Conforming', color='red')\n",
    "plt.axvline(threshold_value, color='blue', linestyle='dashed', linewidth=1, label='Threshold')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'Threshold at {threshold_value:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = np.sum(results[results['conform'] == 1]['distance'] < threshold_value)\n",
    "true_negative = np.sum(results[results['conform'] == 0]['distance'] > threshold_value)\n",
    "false_positive = np.sum(results[results['conform'] == 0]['distance'] < threshold_value)\n",
    "false_negative = np.sum(results[results['conform'] == 1]['distance'] > threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76329c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f1\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f68f8",
   "metadata": {},
   "source": [
    "# Dev (Non Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision for Dev\n",
    "precision = true_negative / (true_negative + false_negative)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e49928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall for Dev\n",
    "recall = true_negative / (true_negative + false_positive)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a18790",
   "metadata": {},
   "source": [
    "# No Dev (Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f08058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision for No Dev\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c323ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall for No Dev\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035c716",
   "metadata": {},
   "source": [
    "# AUC_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'results' is your DataFrame and 'distance' is the score to predict conformity\n",
    "\n",
    "# Inverting the 'distance' scores because higher scores indicate non-conformity\n",
    "# We invert the scores for ROC AUC calculation because roc_auc_score expects higher values\n",
    "# to indicate higher likelihood of the positive class\n",
    "inverted_scores = 1 - results['distance']\n",
    "\n",
    "# Calculate the ROC curve and AUC using inverted scores\n",
    "fpr, tpr, thresholds = roc_curve(results['conform'], inverted_scores, pos_label=1)\n",
    "roc_auc = roc_auc_score(results['conform'], inverted_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='orange', label=f'ROC curve (area = {roc_auc:0.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
