{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7468e092",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099660fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pm4py in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (2.7.7)\n",
      "Requirement already satisfied: deprecation in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (2.1.0)\n",
      "Requirement already satisfied: graphviz in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (0.20.1)\n",
      "Requirement already satisfied: intervaltree in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (3.1.0)\n",
      "Requirement already satisfied: lxml in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (4.9.3)\n",
      "Requirement already satisfied: matplotlib in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (3.8.0)\n",
      "Requirement already satisfied: networkx in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (3.1)\n",
      "Requirement already satisfied: numpy in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (1.26.0)\n",
      "Requirement already satisfied: pandas in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (2.1.1)\n",
      "Requirement already satisfied: pydotplus in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (2.0.2)\n",
      "Requirement already satisfied: pytz in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (2023.3.post1)\n",
      "Requirement already satisfied: scipy in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (1.11.3)\n",
      "Requirement already satisfied: stringdist in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (1.0.9)\n",
      "Requirement already satisfied: tqdm in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (4.66.1)\n",
      "Requirement already satisfied: cvxopt in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pm4py) (1.3.2)\n",
      "Requirement already satisfied: packaging in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from deprecation->pm4py) (23.1)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from intervaltree->pm4py) (2.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from matplotlib->pm4py) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from matplotlib->pm4py) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from matplotlib->pm4py) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from matplotlib->pm4py) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from matplotlib->pm4py) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from matplotlib->pm4py) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from matplotlib->pm4py) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from pandas->pm4py) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->pm4py) (1.16.0)\n",
      "Requirement already satisfied: kneed in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (0.8.5)\n",
      "Requirement already satisfied: numpy>=1.14.2 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from kneed) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from kneed) (1.11.3)\n",
      "Requirement already satisfied: torch in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "\n",
    "# install once necessary libraries\n",
    "\n",
    "!pip install pandas\n",
    "!pip install -U scikit-learn\n",
    "!pip install pm4py\n",
    "!pip install kneed\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e7954",
   "metadata": {},
   "source": [
    "# Import Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ae4a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n",
      "/Users/mert2/.conda/envs/thesis/lib/python3.11/site-packages/pm4py/objects/log/util/dataframe_utils.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], utc=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>travel_start</th>\n",
       "      <th>travel_end</th>\n",
       "      <th>case</th>\n",
       "      <th>cost</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>@@index</th>\n",
       "      <th>@@case_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>17.01.17 11:23</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>17.01.17 11:24</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>17.01.17 11:24</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>17.01.17 11:40</td>\n",
       "      <td>Travel Department</td>\n",
       "      <td>KS9688</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>18.01.17 06:31</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55804</th>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>22.11.17 06:50</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>55804</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55805</th>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>22.11.17 13:06</td>\n",
       "      <td>Manager</td>\n",
       "      <td>AK7488</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>55805</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55806</th>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>29.11.17 20:24</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>55806</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55807</th>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>12.08.17 09:55</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>55807</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55808</th>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>18.12.17 08:52</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>55808</td>\n",
       "      <td>3353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55809 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                activity  \\\n",
       "0                                    file travel request   \n",
       "1      check if travel request needs preliminary pric...   \n",
       "2                        decide on approval requirements   \n",
       "3                          check if booking is necessary   \n",
       "4                       check if expense documents exist   \n",
       "...                                                  ...   \n",
       "55804                      confirm travel expense report   \n",
       "55805                  decide on travel expense approval   \n",
       "55806                 send original documents to archive   \n",
       "55807                                 calculate payments   \n",
       "55808                                       pay expenses   \n",
       "\n",
       "                          start             end               type    user  \\\n",
       "0     2017-01-17 11:17:00+00:00  17.01.17 11:23           Employee  JB8510   \n",
       "1     2017-01-17 11:23:00+00:00  17.01.17 11:24           Employee  JB8510   \n",
       "2     2017-01-17 11:24:00+00:00  17.01.17 11:24           Employee  JB8510   \n",
       "3     2017-01-17 11:24:00+00:00  17.01.17 11:40  Travel Department  KS9688   \n",
       "4     2017-01-18 05:59:00+00:00  18.01.17 06:31           Employee  JB8510   \n",
       "...                         ...             ...                ...     ...   \n",
       "55804 2017-11-22 06:48:00+00:00  22.11.17 06:50           Employee  KI9211   \n",
       "55805 2017-11-22 12:59:00+00:00  22.11.17 13:06            Manager  AK7488   \n",
       "55806 2017-11-29 20:12:00+00:00  29.11.17 20:24           Employee  KI9211   \n",
       "55807 2017-12-08 09:32:00+00:00  12.08.17 09:55         Accounting  FQ3758   \n",
       "55808 2017-12-18 08:39:00+00:00  18.12.17 08:52         Accounting  FQ3758   \n",
       "\n",
       "                   travel_start                travel_end  case  cost  \\\n",
       "0     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "1     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "2     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "3     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "4     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "...                         ...                       ...   ...   ...   \n",
       "55804 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55805 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55806 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55807 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55808 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "\n",
       "      case:concept:name                                       concept:name  \\\n",
       "0                   105                                file travel request   \n",
       "1                   105  check if travel request needs preliminary pric...   \n",
       "2                   105                    decide on approval requirements   \n",
       "3                   105                      check if booking is necessary   \n",
       "4                   105                   check if expense documents exist   \n",
       "...                 ...                                                ...   \n",
       "55804              6348                      confirm travel expense report   \n",
       "55805              6348                  decide on travel expense approval   \n",
       "55806              6348                 send original documents to archive   \n",
       "55807              6348                                 calculate payments   \n",
       "55808              6348                                       pay expenses   \n",
       "\n",
       "                 time:timestamp  @@index  @@case_index  \n",
       "0     2017-01-17 11:17:00+00:00        0             0  \n",
       "1     2017-01-17 11:23:00+00:00        1             0  \n",
       "2     2017-01-17 11:24:00+00:00        2             0  \n",
       "3     2017-01-17 11:24:00+00:00        3             0  \n",
       "4     2017-01-18 05:59:00+00:00        4             0  \n",
       "...                         ...      ...           ...  \n",
       "55804 2017-11-22 06:48:00+00:00    55804          3353  \n",
       "55805 2017-11-22 12:59:00+00:00    55805          3353  \n",
       "55806 2017-11-29 20:12:00+00:00    55806          3353  \n",
       "55807 2017-12-08 09:32:00+00:00    55807          3353  \n",
       "55808 2017-12-18 08:39:00+00:00    55808          3353  \n",
       "\n",
       "[55809 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the CSV file\n",
    "    dataframe = pd.read_csv('mobis.csv', sep=',')  \n",
    "\n",
    "    # Drop the first column without knowing its name\n",
    "    dataframe = dataframe.drop(dataframe.columns[0], axis=1)\n",
    "\n",
    "    # Format the dataframe\n",
    "    dataframe = pm4py.format_dataframe(\n",
    "        dataframe, \n",
    "        case_id='case', \n",
    "        activity_key='activity', \n",
    "        timestamp_key='start'\n",
    "    )\n",
    "\n",
    "    # Convert the dataframe to event log\n",
    "    log = log_converter.apply(dataframe)\n",
    "    \n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198eb53",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b64fa",
   "metadata": {},
   "source": [
    "## Integer Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81331e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>travel_start</th>\n",
       "      <th>travel_end</th>\n",
       "      <th>case</th>\n",
       "      <th>cost</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>@@index</th>\n",
       "      <th>@@case_index</th>\n",
       "      <th>activity_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>17.01.17 11:23</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>file travel request</td>\n",
       "      <td>2017-01-17 11:17:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>17.01.17 11:24</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if travel request needs preliminary pric...</td>\n",
       "      <td>2017-01-17 11:23:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>17.01.17 11:24</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>decide on approval requirements</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>17.01.17 11:40</td>\n",
       "      <td>Travel Department</td>\n",
       "      <td>KS9688</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if booking is necessary</td>\n",
       "      <td>2017-01-17 11:24:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>18.01.17 06:31</td>\n",
       "      <td>Employee</td>\n",
       "      <td>JB8510</td>\n",
       "      <td>2017-10-01 00:00:00+00:00</td>\n",
       "      <td>2017-01-15 00:00:00+00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>check if expense documents exist</td>\n",
       "      <td>2017-01-18 05:59:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55804</th>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>22.11.17 06:50</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>confirm travel expense report</td>\n",
       "      <td>2017-11-22 06:48:00+00:00</td>\n",
       "      <td>55804</td>\n",
       "      <td>3353</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55805</th>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>22.11.17 13:06</td>\n",
       "      <td>Manager</td>\n",
       "      <td>AK7488</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>decide on travel expense approval</td>\n",
       "      <td>2017-11-22 12:59:00+00:00</td>\n",
       "      <td>55805</td>\n",
       "      <td>3353</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55806</th>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>29.11.17 20:24</td>\n",
       "      <td>Employee</td>\n",
       "      <td>KI9211</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>send original documents to archive</td>\n",
       "      <td>2017-11-29 20:12:00+00:00</td>\n",
       "      <td>55806</td>\n",
       "      <td>3353</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55807</th>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>12.08.17 09:55</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>calculate payments</td>\n",
       "      <td>2017-12-08 09:32:00+00:00</td>\n",
       "      <td>55807</td>\n",
       "      <td>3353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55808</th>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>18.12.17 08:52</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>FQ3758</td>\n",
       "      <td>2017-11-19 00:00:00+00:00</td>\n",
       "      <td>2017-11-20 00:00:00+00:00</td>\n",
       "      <td>6348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6348</td>\n",
       "      <td>pay expenses</td>\n",
       "      <td>2017-12-18 08:39:00+00:00</td>\n",
       "      <td>55808</td>\n",
       "      <td>3353</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55809 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                activity  \\\n",
       "0                                    file travel request   \n",
       "1      check if travel request needs preliminary pric...   \n",
       "2                        decide on approval requirements   \n",
       "3                          check if booking is necessary   \n",
       "4                       check if expense documents exist   \n",
       "...                                                  ...   \n",
       "55804                      confirm travel expense report   \n",
       "55805                  decide on travel expense approval   \n",
       "55806                 send original documents to archive   \n",
       "55807                                 calculate payments   \n",
       "55808                                       pay expenses   \n",
       "\n",
       "                          start             end               type    user  \\\n",
       "0     2017-01-17 11:17:00+00:00  17.01.17 11:23           Employee  JB8510   \n",
       "1     2017-01-17 11:23:00+00:00  17.01.17 11:24           Employee  JB8510   \n",
       "2     2017-01-17 11:24:00+00:00  17.01.17 11:24           Employee  JB8510   \n",
       "3     2017-01-17 11:24:00+00:00  17.01.17 11:40  Travel Department  KS9688   \n",
       "4     2017-01-18 05:59:00+00:00  18.01.17 06:31           Employee  JB8510   \n",
       "...                         ...             ...                ...     ...   \n",
       "55804 2017-11-22 06:48:00+00:00  22.11.17 06:50           Employee  KI9211   \n",
       "55805 2017-11-22 12:59:00+00:00  22.11.17 13:06            Manager  AK7488   \n",
       "55806 2017-11-29 20:12:00+00:00  29.11.17 20:24           Employee  KI9211   \n",
       "55807 2017-12-08 09:32:00+00:00  12.08.17 09:55         Accounting  FQ3758   \n",
       "55808 2017-12-18 08:39:00+00:00  18.12.17 08:52         Accounting  FQ3758   \n",
       "\n",
       "                   travel_start                travel_end  case  cost  \\\n",
       "0     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "1     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "2     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "3     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "4     2017-10-01 00:00:00+00:00 2017-01-15 00:00:00+00:00   105   NaN   \n",
       "...                         ...                       ...   ...   ...   \n",
       "55804 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55805 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55806 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55807 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "55808 2017-11-19 00:00:00+00:00 2017-11-20 00:00:00+00:00  6348   NaN   \n",
       "\n",
       "      case:concept:name                                       concept:name  \\\n",
       "0                   105                                file travel request   \n",
       "1                   105  check if travel request needs preliminary pric...   \n",
       "2                   105                    decide on approval requirements   \n",
       "3                   105                      check if booking is necessary   \n",
       "4                   105                   check if expense documents exist   \n",
       "...                 ...                                                ...   \n",
       "55804              6348                      confirm travel expense report   \n",
       "55805              6348                  decide on travel expense approval   \n",
       "55806              6348                 send original documents to archive   \n",
       "55807              6348                                 calculate payments   \n",
       "55808              6348                                       pay expenses   \n",
       "\n",
       "                 time:timestamp  @@index  @@case_index  activity_encoded  \n",
       "0     2017-01-17 11:17:00+00:00        0             0                15  \n",
       "1     2017-01-17 11:23:00+00:00        1             0                 7  \n",
       "2     2017-01-17 11:24:00+00:00        2             0                11  \n",
       "3     2017-01-17 11:24:00+00:00        3             0                 3  \n",
       "4     2017-01-18 05:59:00+00:00        4             0                 4  \n",
       "...                         ...      ...           ...               ...  \n",
       "55804 2017-11-22 06:48:00+00:00    55804          3353                 8  \n",
       "55805 2017-11-22 12:59:00+00:00    55805          3353                13  \n",
       "55806 2017-11-29 20:12:00+00:00    55806          3353                21  \n",
       "55807 2017-12-08 09:32:00+00:00    55807          3353                 1  \n",
       "55808 2017-12-18 08:39:00+00:00    55808          3353                17  \n",
       "\n",
       "[55809 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "dataframe['activity_encoded'] = label_encoder.fit_transform(dataframe['activity'])\n",
    "\n",
    "# Now the 'dataframe' has a new column 'activity_encoded' with integer encoded values of the 'activity' column\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2687d00",
   "metadata": {},
   "source": [
    "## Extract Traces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb77562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@@case_index\n",
       "0             [15, 7, 11, 3, 4, 25, 14, 8, 13, 21, 1, 17]\n",
       "1       [15, 7, 18, 20, 2, 5, 19, 18, 20, 2, 24, 11, 3...\n",
       "2       [15, 7, 11, 3, 18, 20, 2, 0, 4, 25, 14, 8, 13,...\n",
       "3       [15, 7, 11, 3, 18, 20, 2, 6, 19, 18, 20, 2, 0,...\n",
       "4       [15, 7, 18, 20, 2, 24, 11, 3, 4, 14, 8, 13, 23...\n",
       "                              ...                        \n",
       "3349              [15, 7, 11, 3, 4, 14, 8, 13, 21, 1, 17]\n",
       "3350    [15, 7, 11, 3, 4, 25, 14, 8, 13, 23, 10, 13, 2...\n",
       "3351    [15, 7, 11, 16, 12, 3, 4, 25, 14, 8, 13, 21, 1...\n",
       "3352    [15, 7, 11, 16, 12, 3, 4, 25, 14, 8, 13, 21, 1...\n",
       "3353              [15, 7, 11, 3, 4, 14, 8, 13, 21, 1, 17]\n",
       "Name: activity_encoded, Length: 3354, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by case index to get individual traces\n",
    "traces = dataframe.groupby('@@case_index')['activity_encoded'].apply(list)\n",
    "traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a56959",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8246aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15,  7, 11,  ...,  0,  0,  0],\n",
       "        [15,  7, 18,  ...,  0,  0,  0],\n",
       "        [15,  7, 11,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [15,  7, 11,  ...,  0,  0,  0],\n",
       "        [15,  7, 11,  ...,  0,  0,  0],\n",
       "        [15,  7, 11,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post-Padding\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# First, we convert the pandas Series 'traces' to a list of PyTorch tensors\n",
    "traces_tensors = [torch.tensor(trace) for trace in traces]\n",
    "\n",
    "# Then, we apply padding to this list of tensors\n",
    "# We are assuming here that your traces are sequences of integers that represent encoded activities\n",
    "# pad_sequence requires the input to be a list (or other iterable) of tensors\n",
    "padded_traces = pad_sequence(traces_tensors, batch_first=True)\n",
    "\n",
    "# Now 'padded_traces' is a tensor with each row representing a trace and zeros padding the sequences to equal length\n",
    "padded_traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0456bb2",
   "metadata": {},
   "source": [
    "# Build Input X and output Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f37463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'padded_traces' is already created as per the previous discussion.\n",
    "\n",
    "# Determine the integer for 'END' token, assuming it's the max(encoded_activities) + 1\n",
    "end_token = padded_traces.max() + 1\n",
    "\n",
    "# Initialize lists to hold the input sequences X and the targets Y\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Iterate through each trace\n",
    "for trace in padded_traces:\n",
    "    # Initialize the sequence with the 'null' token\n",
    "    seq = [0]  \n",
    "    for activity in trace:\n",
    "        # Exclude padding (0) and end token (if already included)\n",
    "        if activity.item() not in (0, end_token):\n",
    "            # Append the current sequence to X\n",
    "            X.append(seq.copy())\n",
    "            # Append the next activity as the target to Y\n",
    "            Y.append(activity.item())\n",
    "            # Add the current activity to the sequence\n",
    "            seq.append(activity.item())\n",
    "    # After the last activity in the trace, append the 'END' token to the sequence and to Y\n",
    "    X.append(seq.copy())\n",
    "    Y.append(end_token)\n",
    "\n",
    "# Convert X and Y to PyTorch tensors\n",
    "X_tensor = pad_sequence([torch.tensor(x) for x in X], batch_first=True, padding_value=0)\n",
    "Y_tensor = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19fa48f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([57673, 49])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8ed440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([57673])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc99776",
   "metadata": {},
   "source": [
    "# Optional: Prepare Test and Train Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e6cd0dc",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert tensors to numpy arrays for train_test_split function\n",
    "X_numpy = X_tensor.numpy()\n",
    "Y_numpy = Y_tensor.numpy()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_numpy, Y_numpy, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert them back to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long).to(device)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a44b981",
   "metadata": {},
   "source": [
    "X_tensor = X_train_tensor\n",
    "Y_tensor = Y_train_tensor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85c62037",
   "metadata": {},
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2112b0bf",
   "metadata": {},
   "source": [
    "Y_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264de28",
   "metadata": {},
   "source": [
    "# LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff032e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_activities, embedding_dim, hidden_dim, output_dim, dropout_prob=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Output of the last LSTM cell\n",
    "        return out, h_n\n",
    "\n",
    "# Model parameters\n",
    "num_activities = end_token + 1 # define this based on your data\n",
    "embedding_dim = len(dataframe['activity'].unique())\n",
    "hidden_dim = 100\n",
    "output_dim = num_activities\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lstm_model = LSTMModel(num_activities, embedding_dim, hidden_dim, output_dim, dropout_prob=0.2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c97a8f",
   "metadata": {},
   "source": [
    "# LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters())\n",
    "\n",
    "# Data preparation\n",
    "train_data = TensorDataset(X_tensor, Y_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = lstm_model(inputs)  # Ignore hidden states during training\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c448e",
   "metadata": {},
   "source": [
    "# Optional: Evaluation of LSTM Predictive Performance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15602d20",
   "metadata": {},
   "source": [
    "# Test data loader\n",
    "test_data = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=32)  # Adjust the batch size if necessary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fab5d922",
   "metadata": {},
   "source": [
    "# Put the model in evaluation mode\n",
    "lstm_model.eval()\n",
    "\n",
    "# Initialize variables to track predictions and actuals\n",
    "total_predictions = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "# Disable gradient computation during evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs, _ = lstm_model(inputs)\n",
    "\n",
    "        # Convert outputs to predicted class labels\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update total and correct predictions\n",
    "        total_predictions += targets.size(0)\n",
    "        correct_predictions += (predicted_labels == targets).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Accuracy for predicting next activity: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bba543",
   "metadata": {},
   "source": [
    "# Extract Trace Representations Using LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To collect hidden states for the entire dataset\n",
    "lstm_model.eval()  # Set the model to evaluation mode\n",
    "hidden_states = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs in DataLoader(X_tensor, batch_size=1):  # Process one sequence at a time\n",
    "        inputs = inputs.to(device)\n",
    "        _, h_n = lstm_model(inputs)  # Get the hidden state\n",
    "        hidden_states.append(h_n.squeeze(0).cpu().numpy())  # Remove batch dimension and convert to numpy\n",
    "\n",
    "hidden_states = np.array(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87771f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = hidden_states.squeeze(1)  # Removes the singleton dimension\n",
    "\n",
    "# Check the new shape\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by case index to get individual traces\n",
    "traces = dataframe.groupby('@@case_index')['activity'].apply(list)\n",
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_pos = traces.apply(len).values\n",
    "hidden_state_pos = hidden_state_pos - 1\n",
    "hidden_state_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_representations = [hidden_states[i-1] for i in hidden_state_pos]\n",
    "trace_representations = np.array(trace_representations)\n",
    "len(trace_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4245f4",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33175a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Elbow method to determine the optimal number of clusters\n",
    "distortions = []\n",
    "K = range(1, 10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(trace_representations)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Find the elbow point\n",
    "kl = KneeLocator(range(1, 10), distortions, curve=\"convex\", direction=\"decreasing\")\n",
    "print(\"Optimal number of clusters:\", kl.elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737848c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "k = kl.elbow\n",
    "\n",
    "# Create a KMeans instance with k clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(trace_representations)\n",
    "\n",
    "# Predict the clusters for each trace\n",
    "clusters = kmeans.predict(trace_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f4834",
   "metadata": {},
   "source": [
    "# Cluster Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea21f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value of the silhouette coefficient ranges between -1 and 1\n",
    "# value close to 1 is considered as good\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(trace_representations, clusters)\n",
    "\n",
    "print(f\"Silhouette Coefficient: {silhouette_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0556d66",
   "metadata": {},
   "source": [
    "# Conformance Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056854ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments_petri\n",
    "\n",
    "# 2. Import the given BPMN model\n",
    "bpmn_graph = bpmn_importer.apply(\"MobisToBe.bpmn\")\n",
    "\n",
    "# 3. Convert the BPMN to a Petri net\n",
    "net, im, fm = pm4py.convert_to_petri_net(bpmn_graph)\n",
    "\n",
    "# 4. Perform alignment-based conformance checking\n",
    "alignments = alignments_petri.apply(log, net, im, fm)\n",
    "\n",
    "# Calculate and print diagnostics\n",
    "fit_traces = sum(1 for trace in alignments if trace['fitness'] == 1.0)\n",
    "\n",
    "print(f\"Total traces: {len(log)}\")\n",
    "print(f\"Conform traces: {fit_traces}\")\n",
    "print(f\"Non-Conform traces: {len(log) - fit_traces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255155be",
   "metadata": {},
   "source": [
    "# Input Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = ['file travel request', 'check if travel request needs preliminary price inquiry', 'decide on approval requirements', 'check if booking is necessary', 'check if expense documents exist', 'upload travel expense documents', 'file travel expense report', 'confirm travel expense report', 'decide on travel expense approval', 'send original documents to archive', 'calculate payments', 'pay expenses']\n",
    "trace2 = ['file travel request', 'check if travel request needs preliminary price inquiry', 'decide on approval requirements', 'check if booking is necessary', 'check if expense documents exist', 'file travel expense report', 'confirm travel expense report', 'decide on travel expense approval', 'send original documents to archive', 'calculate payments', 'pay expenses']\n",
    "trace3 = ['file travel request', 'check if travel request needs preliminary price inquiry', 'decide on approval requirements', 'forward request to approver', 'decide on request', 'check if booking is necessary', 'check if expense documents exist', 'upload travel expense documents', 'file travel expense report', 'confirm travel expense report', 'decide on travel expense approval', 'send original documents to archive', 'calculate payments', 'pay expenses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8156b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = dataframe.groupby('@@case_index')['concept:name'].apply(list).reset_index(name='trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c997b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_happy_trace(row_trace):\n",
    "    predefined_traces = [trace1, trace2, trace3]\n",
    "    for trace in predefined_traces:\n",
    "        if row_trace == trace:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d24636",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['happy'] = grouped['trace'].apply(is_happy_trace)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of the happy traces in the results dataframe\n",
    "happy_trace_indices = grouped[grouped['happy'] == 1].index.tolist()\n",
    "\n",
    "# Extract the corresponding coordinates from the trace_representations array\n",
    "happy_trace_coordinates = trace_representations[happy_trace_indices]\n",
    "\n",
    "# Extract unique coordinates\n",
    "unique_happy_trace_coordinates = np.unique(happy_trace_coordinates, axis=0)\n",
    "\n",
    "# Assuming the size of unique_happy_trace_coordinates is 3\n",
    "#happy_trace1, happy_trace2, happy_trace3 = unique_happy_trace_coordinates\n",
    "happy_trace1, happy_trace2, happy_trace3 = unique_happy_trace_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108b9a5",
   "metadata": {},
   "source": [
    "# Distance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1997c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Calculate the distances to each of the happy traces for every trace representation\n",
    "distances_to_happy_traces = []\n",
    "\n",
    "for trace_representation in trace_representations:\n",
    "    distances = [\n",
    "        euclidean(trace_representation, happy_trace1),\n",
    "        euclidean(trace_representation, happy_trace2),\n",
    "        euclidean(trace_representation, happy_trace3)\n",
    "    ]\n",
    "    distances_to_happy_traces.append(distances)\n",
    "\n",
    "# Calculate the average distance to the happy traces for each trace representation\n",
    "avg_distances = [np.mean(distances) for distances in distances_to_happy_traces]\n",
    "\n",
    "# Save the distances in a variable\n",
    "avg_distances_var = np.array(avg_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac090cc2",
   "metadata": {},
   "source": [
    "# Results overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f020556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with necessary information for distance measurement\n",
    "\n",
    "#grouped = dataframe.groupby('@@case_index')['concept:name'].apply(list).reset_index(name='trace')\n",
    "\n",
    "results = pd.DataFrame(grouped['trace'])\n",
    "\n",
    "results['cluster'] = clusters\n",
    "\n",
    "conformity_array = [int(trace['fitness']) for trace in alignments]\n",
    "results['conform'] = conformity_array\n",
    "\n",
    "results['distance'] = avg_distances_var\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = results.groupby('cluster').agg(\n",
    "    count=pd.NamedAgg(column='trace', aggfunc='size'),\n",
    "    conform_count=pd.NamedAgg(column='conform', aggfunc='sum')\n",
    ").reset_index()\n",
    "\n",
    "print(\"cluster\\tcount\\tconform_count\")\n",
    "for _, row in summary.iterrows():\n",
    "    print(f\"{row['cluster']}\\t{row['count']}\\t{row['conform_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1ab2e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Filter the DataFrame into conforming and non-conforming subsets\n",
    "conforming_distances = results[results['conform'] == 1]['distance']\n",
    "non_conforming_distances = results[results['conform'] == 0]['distance']\n",
    "\n",
    "# Determine common bin edges\n",
    "min_distance = min(results['distance'])\n",
    "max_distance = max(results['distance'])\n",
    "bin_edges = np.linspace(min_distance, max_distance, num=30)\n",
    "\n",
    "# Combine the data and reshape for k-means\n",
    "all_distances = results['distance']\n",
    "all_distances = np.array(all_distances)\n",
    "all_distances_reshaped = all_distances.reshape(-1, 1)\n",
    "\n",
    "# Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(all_distances_reshaped)\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Find the threshold as the average of the two cluster centers\n",
    "threshold_value = np.mean(kmeans.cluster_centers_)\n",
    "\n",
    "# Plot histograms and the threshold\n",
    "plt.hist(conforming_distances, bins=bin_edges, alpha=0.5, label='Conforming', color='green')\n",
    "plt.hist(non_conforming_distances, bins=bin_edges, alpha=0.5, label='Non-Conforming', color='red')\n",
    "plt.axvline(threshold_value, color='blue', linestyle='dashed', linewidth=1, label='Threshold')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'Threshold at {threshold_value:.2f}')\n",
    "\n",
    "plt.savefig('Mobis_Threshold.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = np.sum(results[results['conform'] == 1]['distance'] < threshold_value)\n",
    "true_negative = np.sum(results[results['conform'] == 0]['distance'] > threshold_value)\n",
    "false_positive = np.sum(results[results['conform'] == 0]['distance'] < threshold_value)\n",
    "false_negative = np.sum(results[results['conform'] == 1]['distance'] > threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76329c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f1\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b08e29",
   "metadata": {},
   "source": [
    "# Dev (Non Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85409078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision for Dev\n",
    "precision = true_negative / (true_negative + false_negative)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall for Dev\n",
    "recall = true_negative / (true_negative + false_positive)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bfbc39",
   "metadata": {},
   "source": [
    "# No Dev (Conform Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision for No Dev\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "print(f\"Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall for No Dev\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56874292",
   "metadata": {},
   "source": [
    "# AUC_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['prediction'] = results['distance'].apply(lambda x: 1 if x < threshold_value else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a462a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'results' is your DataFrame and 'distance' is the score to predict conformity\n",
    "\n",
    "# Inverting the 'distance' scores because higher scores indicate non-conformity\n",
    "# We invert the scores for ROC AUC calculation because roc_auc_score expects higher values\n",
    "# to indicate higher likelihood of the positive class\n",
    "inverted_scores = 1 - results['distance']\n",
    "\n",
    "# Calculate the ROC curve and AUC using inverted scores\n",
    "fpr, tpr, thresholds = roc_curve(results['conform'], inverted_scores, pos_label=1)\n",
    "roc_auc = roc_auc_score(results['conform'], inverted_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='orange', label=f'ROC curve (area = {roc_auc:0.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
